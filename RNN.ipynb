{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HYAD-Yassin/Password_Factory/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj8X9kXTH5Uy"
      },
      "source": [
        "#**Premiere Base de Donn√©es**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ-B_dStHmih",
        "outputId": "5d0405d6-9445-47c6-ff20-3438bc93d490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            " 100K_LSTM_passwords.txt\n",
            " 100K_RNN_Passwords.txt\n",
            " 10K_LSTM_passwords.txt\n",
            " 1M_LSTM_passwords.txt\n",
            " acoustique_voy_orales_20loc_ESTER_NCCFr_contexte_freqLex_distCentroide.csv\n",
            " AllDataSet_Filtred.txt\n",
            " AllDataSet.txt\n",
            "'Alternance (1).gdoc'\n",
            " Alternance.gdoc\n",
            " archive.zip\n",
            " Ashley-Madison_Ini.txt\n",
            " Ashley-Madison.txt\n",
            "'Colab Notebooks'\n",
            " DATABASE_Password.zip\n",
            " data.zip\n",
            " Filtered-Ashley-Madison.txt\n",
            " Filtered_PWD.txt\n",
            " generated100k_GRU_passwords.txt\n",
            " generated100K_LSTM_passwords.txt\n",
            " generated10K_LSTM_passwords.txt\n",
            " generated1M_GRU_passwords.txt\n",
            " generated200K_LSTM_passwords.txt\n",
            " generated50K_LSTM_passwords.txt\n",
            " generated_GRU_passwords.txt\n",
            " generated_LSTM_passwords.txt\n",
            " generated_passwords.txt\n",
            " gru_model.pth\n",
            " histo2.png\n",
            " histogram1.png\n",
            " Letter.gdoc\n",
            " lstm_model2.pth\n",
            " lstm_model.pth\n",
            " my_model\n",
            " nameGeneration.py\n",
            " Passwords.txt\n",
            " pwd_2Rnn.pth\n",
            " pwd_Rnn.pth\n",
            " __pycache__\n",
            " reduced_Ashley-Madison.txt\n",
            " rnn.pt\n",
            "'Untitled document (1).gdoc'\n",
            "'Untitled document.gdoc'\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GZCJGbQVJXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d66606ec-a3cc-44d4-e2e6-fadf0b4f65fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Read the file and prepare the data\n",
        "file_path = '/content/drive/My Drive/Ashley-Madison.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "# Create a mapping of characters to integers\n",
        "chars = sorted(list(set(data)))\n",
        "char_to_int = {c: i for i, c in enumerate(chars)}\n",
        "int_to_char = {i: c for i, c in enumerate(chars)}\n",
        "n_characters = len(chars)  # Number of unique characters\n",
        "\n",
        "# Prepare the dataset with integer encoding\n",
        "max_length = 100  # Maximum length of a password\n",
        "step = 3  # Step size for sampling\n",
        "\n",
        "sequences = []\n",
        "for i in range(0, len(data) - max_length, step):\n",
        "    sequences.append(data[i: i + max_length])\n",
        "\n",
        "# Split the data into training (70%) and validation (30%) sets\n",
        "train_sequences, _ = train_test_split(sequences, train_size=0.7, random_state=42)\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Dataset**"
      ],
      "metadata": {
        "id": "13heV4C6Ki80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# The path to your file - replace with the actual path\n",
        "file_path = '/content/drive/My Drive/Ashley-Madison.txt'\n",
        "\n",
        "# Function to filter passwords\n",
        "def filter_passwords(text):\n",
        "    # Splitting text into lines assuming one password per line\n",
        "    lines = text.split('\\n')\n",
        "    # Filtering lines where length of line is between 6 and 15 characters\n",
        "    filtered_lines = [line for line in lines if 6 <= len(line) <= 15]\n",
        "    return '\\n'.join(filtered_lines)\n",
        "\n",
        "# Read the file\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Filter the text\n",
        "filtered_text = filter_passwords(text)\n",
        "\n",
        "# Write the filtered text back to the file\n",
        "with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "k5qU7MPgKiK0",
        "outputId": "01a5e848-3633-4085-978d-ea9c68dd34fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9e5ef9776aa6>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Read the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Ashley-Madison.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ovAzZuIVKS7"
      },
      "source": [
        "#**RNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAsyltMcVKHd"
      },
      "outputs": [],
      "source": [
        "# RNN Class\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, n_layers, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_size, 256)\n",
        "        self.fc2 = nn.Linear(256, output_size)\n",
        "\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # Embedding layer expects input of shape [batch_size, seq_len] with batch_size=1\n",
        "        embedded = self.embedding(input.view(1, -1))\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        output=self.fc1(output)\n",
        "        output = self.fc2(output.view(1, -1))\n",
        "        return output\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.n_layers, 1, self.hidden_size)\n",
        "\n",
        "# Convert string to tensor\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = char_to_int[string[c]]\n",
        "    return tensor.to(device)\n",
        "\n",
        "# Random Training Example\n",
        "def random_training_example():\n",
        "    line = random.choice(train_sequences)\n",
        "    input_tensor = char_tensor(line[:-1])\n",
        "    target_tensor = char_tensor(line[1:])\n",
        "    return input_tensor, target_tensor\n",
        "\n",
        "# Initialize the RNN\n",
        "hidden_size = 512\n",
        "rnn = RNN(n_characters, hidden_size, n_characters).to(device)\n",
        "\n",
        "# Training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(rnn.parameters(), lr=1e-3)\n",
        "\n",
        "# Training function with device support\n",
        "def train(input_tensor, target_tensor):\n",
        "    hidden = rnn.initHidden().to(device)  # Move hidden state to the device\n",
        "\n",
        "    rnn.zero_grad()\n",
        "    loss = 0\n",
        "\n",
        "    input_tensor = input_tensor.to(device)  # Move input tensor to the device\n",
        "    target_tensor = target_tensor.to(device)  # Move target tensor to the device\n",
        "\n",
        "    for i in range(input_tensor.size(0)):\n",
        "        output = rnn(input_tensor[i].unsqueeze(0), hidden)\n",
        "        l = criterion(output, target_tensor[i].unsqueeze(0))\n",
        "        loss += l\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item() / input_tensor.size(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4YGZVYnWIzR"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4evBsYolF5E",
        "outputId": "4ad6332d-f640-4e3a-8fa9-ecc479b6e9ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500 Loss: 3.2301\n",
            "Epoch 1000 Loss: 3.1195\n",
            "Epoch 1500 Loss: 3.0833\n",
            "Epoch 2000 Loss: 3.0896\n",
            "Epoch 2500 Loss: 3.0809\n",
            "Epoch 3000 Loss: 3.0759\n",
            "Epoch 3500 Loss: 3.0705\n",
            "Epoch 4000 Loss: 3.0787\n",
            "Epoch 4500 Loss: 3.0720\n",
            "Epoch 5000 Loss: 3.0676\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "n_epochs = 5000\n",
        "print_every = 500\n",
        "current_loss = 0\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    input_tensor, target_tensor = random_training_example()\n",
        "    loss = train(input_tensor, target_tensor)\n",
        "    current_loss += loss\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        print(f'Epoch {epoch} Loss: {current_loss / print_every:.4f}')\n",
        "        current_loss = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = '/content/drive/My Drive/pwd_2Rnn.pth'  # Modify as needed\n",
        "torch.save(rnn.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "3oEUebOrrVkl",
        "outputId": "ce32ba1a-bd55-491e-9bce-50c08ab266d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c9f75fa6b2ac>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/pwd_2Rnn.pth'\u001b[0m  \u001b[0;31m# Modify as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model saved to {model_save_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "rnn = RNN(n_characters, hidden_size, n_characters).to(device)\n",
        "\n",
        "# Mount Google Drive (specific to Google Colab, you might not need this if your environment already has access to the drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your model file\n",
        "model_save_path = '/content/drive/My Drive/pwd_2Rnn.pth'\n",
        "\n",
        "# Load the model onto CPU\n",
        "rnn.load_state_dict(torch.load(model_save_path, map_location=torch.device('cpu')))\n",
        "print(f\"Model loaded from {model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNs-4J_nSXf9",
        "outputId": "1b84eecf-efe7-4bca-cbee-732591ba6c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model loaded from /content/drive/My Drive/pwd_2Rnn.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QErCeXjtbR4f"
      },
      "source": [
        "**Passwords Generation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VFGO6txp7y1"
      },
      "outputs": [],
      "source": [
        "def generate_password(model, start_str='A', predict_len=100, temperature=0.8):\n",
        "    model.to(device)  # Ensure the model is on the right device\n",
        "    hidden = model.initHidden().to(device)\n",
        "    input_tensor = char_tensor(start_str).to(device)\n",
        "    predicted_str = start_str\n",
        "\n",
        "    for p in range(len(start_str) - 1):\n",
        "        _, hidden = model(input_tensor[p].unsqueeze(0), hidden)\n",
        "\n",
        "    last_char = input_tensor[-1]\n",
        "\n",
        "    for p in range(predict_len):\n",
        "        output = model(last_char.unsqueeze(0), hidden)\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_char = torch.multinomial(output_dist, 1)[0]\n",
        "\n",
        "        predicted_char = int_to_char[top_char.item()]\n",
        "        predicted_str += predicted_char\n",
        "        last_char = char_tensor(predicted_char)\n",
        "\n",
        "    return predicted_str\n",
        "\n",
        "def generate_passwords_to_file(model, num_pwd, file_path, start_str='A', predict_len=100, temperature=0.85):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for _ in range(num_pwd):\n",
        "            password = generate_password(model, start_str, predict_len, temperature)\n",
        "            file.write(password + '\\n')\n",
        "\n",
        "# Example usage: Generate a specified number of passwords and save them to a file\n",
        "num_pwd = 100000  # Specify the number of passwords to generate\n",
        "output_file_path = '1M_RNN_passwords.txt'  # Modify this path as needed\n",
        "generate_passwords_to_file(rnn, num_pwd, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '1M_RNN_passwords.txt' '/content/drive/My Drive'"
      ],
      "metadata": {
        "id": "g4ZaafmhssCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mktljP0WNBQ"
      },
      "source": [
        "**Calculate Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYWuW9uVbZHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b342a40d-9a59-4a5d-d77c-1f8256f91d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.00%  Matches / DataSet Initial\n",
            "Matching Passwords Number: 1\n",
            "Matching Passwords: {'020300c'}\n"
          ]
        }
      ],
      "source": [
        "def calculate_accuracy_and_matches(original_dataset_path, generated_file_path):\n",
        "    # Load the original dataset\n",
        "    with open(original_dataset_path, 'r') as file:\n",
        "        original_passwords = set(file.read().splitlines())\n",
        "\n",
        "    # Load generated passwords\n",
        "    with open(generated_file_path, 'r') as file:\n",
        "        generated_passwords = file.read().splitlines()\n",
        "\n",
        "    # Find matches\n",
        "    matches = [password for password in generated_passwords if password in original_passwords]\n",
        "    # Calculate accuracy based on full line matches\n",
        "    accuracy = (len(matches) / len(original_passwords)) * 100 if generated_passwords else 0\n",
        "    return accuracy, matches\n",
        "\n",
        "# Paths to the files\n",
        "original_dataset_path = '/content/drive/My Drive/Ashley-Madison_Ini.txt'  # Modify as needed\n",
        "generated_file_path = '/content/drive/My Drive/generated_GRU_passwords.txt'  # Modify as needed\n",
        "\n",
        "# Calculate accuracy and get matches\n",
        "accuracy, matching_passwords = calculate_accuracy_and_matches(original_dataset_path, generated_file_path)\n",
        "print(f\"Accuracy: {accuracy:.2f}%  Matches / DataSet Initial\")\n",
        "\n",
        "print(\"Matching Passwords Number:\", len(matching_passwords))\n",
        "print(\"Matching Passwords:\", set(matching_passwords))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data\n",
        "with open(generated_file_path, 'r', encoding='utf-8') as file:\n",
        "    passwords = file.read().splitlines()  # Each password is a line\n",
        "\n",
        "print(f\"The total passwords in The DataSet is: {len(passwords)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3R3sZxf_bcV",
        "outputId": "4b8f2446-3a82-44bc-fce8-e0ac72eaa7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total passwords in The DataSet is: 10000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvrCqc3c/Z53kKSrgqEzZa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}