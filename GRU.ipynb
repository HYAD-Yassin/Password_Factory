{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOCPjKUH3vbg62UyEszxw+D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HYAD-Yassin/Password_Factory/blob/main/GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0bypdxaUsGK",
        "outputId": "fd4d7667-cd0f-4831-8937-59e2f2da12a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " acoustique_voy_orales_20loc_ESTER_NCCFr_contexte_freqLex_distCentroide.csv\n",
            " AllDataSet_Filtred.txt\n",
            " AllDataSet.txt\n",
            "'Alternance (1).gdoc'\n",
            " Alternance.gdoc\n",
            " archive.zip\n",
            " Ashley-Madison.txt\n",
            "'Colab Notebooks'\n",
            " DATABASE_Password.zip\n",
            " data.zip\n",
            " Filtered-Ashley-Madison.txt\n",
            " Filtered_PWD.txt\n",
            " generated100k_GRU_passwords.txt\n",
            " generated1M_GRU_passwords.txt\n",
            " generated2_passwords.txt\n",
            " generated_GRU_passwords.txt\n",
            " generated_LSTM_passwords.txt\n",
            " generated_passwords.txt\n",
            " gru_model.pth\n",
            " histo2.png\n",
            " histogram1.png\n",
            " Letter.gdoc\n",
            " my_model\n",
            " nameGeneration.py\n",
            " Passwords.txt\n",
            " pwd_2Rnn.pth\n",
            " pwd_Rnn.pth\n",
            " __pycache__\n",
            " reduced_Ashley-Madison.txt\n",
            " rnn.pt\n",
            "'Untitled document (1).gdoc'\n",
            "'Untitled document.gdoc'\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "# Define the file path\n",
        "file_path = '/content/drive/My Drive/Ashley-Madison.txt'\n",
        "\n",
        "# Read the passwords\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    passwords = file.read().splitlines()\n",
        "\n",
        "# Group passwords by their first character\n",
        "passwords_by_first_char = defaultdict(list)\n",
        "for pwd in passwords:\n",
        "    first_char = pwd[0]\n",
        "    passwords_by_first_char[first_char].append(pwd)\n",
        "\n",
        "# Calculate how many passwords to take from each group\n",
        "desired_total = 60000  # Target number of passwords\n",
        "groups_count = len(passwords_by_first_char)  # Number of different starting characters\n",
        "passwords_per_group = desired_total // groups_count  # Evenly distribute the desired total across the groups\n",
        "\n",
        "# Sample from each group\n",
        "reduced_passwords = []\n",
        "for char, pwd_list in passwords_by_first_char.items():\n",
        "    # If there are fewer passwords than the target per group, take them all; otherwise, take a sample.\n",
        "    if len(pwd_list) <= passwords_per_group:\n",
        "        reduced_passwords.extend(pwd_list)\n",
        "    else:\n",
        "        reduced_passwords.extend(np.random.choice(pwd_list, passwords_per_group, replace=False))\n",
        "\n",
        "# Output the reduced list of passwords\n",
        "with open('/content/drive/My Drive/reduced_Ashley-Madison.txt', 'w') as file:\n",
        "    for pwd in reduced_passwords:\n",
        "        file.write(f\"{pwd}\\n\")\n"
      ],
      "metadata": {
        "id": "W9RkKDzMfpJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter\n",
        "import os\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Check if CUDA is available, else use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "file_path = '/content/drive/My Drive/reduced_Ashley-Madison.txt'\n",
        "#file_path = '/content/drive/My Drive/Ashley-Madison.txt'\n",
        "\n",
        "# Read the data\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    passwords = file.read().splitlines()  # Each password is a line\n",
        "\n",
        "print(f\"The total passwords in The DataSet is: {len(passwords)}\")\n",
        "\n",
        "\n",
        "# Preprocessing: Create a dictionary to convert characters to integers and back\n",
        "all_chars = ''.join(set(''.join(passwords)))\n",
        "n_characters = len(all_chars)\n",
        "char_to_int = {char: i for i, char in enumerate(all_chars)}\n",
        "int_to_char = {i: char for i, char in enumerate(all_chars)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HHUjyJ9UyxJ",
        "outputId": "2f87c2fc-173d-4f38-ecf5-be3f8d89e35f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total passwords in The DataSet is: 38839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRU Model\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input.view(1, -1))\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output = self.fc(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(self.n_layers, 1, self.hidden_size).to(device)\n"
      ],
      "metadata": {
        "id": "snxXRCE1VF-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert string to tensor\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = char_to_int[string[c]]\n",
        "    return tensor\n",
        "# Training function that uses the entire dataset\n",
        "def train_on_full_dataset(passwords):\n",
        "    total_loss = 0\n",
        "    for password in passwords:\n",
        "        input_line_tensor = char_tensor(password[:-1])\n",
        "        target_line_tensor = char_tensor(password[1:])\n",
        "        hidden = gru.init_hidden()\n",
        "\n",
        "        gru.zero_grad()\n",
        "        loss = 0\n",
        "\n",
        "        for i in range(input_line_tensor.size(0)):\n",
        "            output, hidden = gru(input_line_tensor[i].unsqueeze(0).to(device), hidden)\n",
        "            l = criterion(output, target_line_tensor[i].unsqueeze(0).to(device))\n",
        "            loss += l\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() / input_line_tensor.size(0)\n",
        "\n",
        "    return total_loss / len(passwords)  # Return the average loss\n",
        "\n",
        "# Initialize network, optimizer, and loss function as before\n",
        "hidden_size = 128\n",
        "n_layers = 2\n",
        "gru = GRU(n_characters, hidden_size, n_characters, n_layers).to(device)\n",
        "optimizer = optim.Adam(gru.parameters(), lr=0.005)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "n_epochs = 10  # Number of times you go through the entire dataset\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    loss = train_on_full_dataset(passwords)\n",
        "    print(f'Epoch: {epoch} of {n_epochs}, Loss: {loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5yKHU_bgkyA",
        "outputId": "67a96f82-3893-44fc-d1dd-a8fc6f885506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 of 10, Loss: 3.0577\n",
            "Epoch: 2 of 10, Loss: 3.0562\n",
            "Epoch: 3 of 10, Loss: 3.0516\n",
            "Epoch: 4 of 10, Loss: 3.0489\n",
            "Epoch: 5 of 10, Loss: 3.0479\n",
            "Epoch: 6 of 10, Loss: 3.0482\n",
            "Epoch: 7 of 10, Loss: 3.0459\n",
            "Epoch: 8 of 10, Loss: 3.0451\n",
            "Epoch: 9 of 10, Loss: 3.0424\n",
            "Epoch: 10 of 10, Loss: 3.0436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = GRU(n_characters, hidden_size, n_characters).to(device)"
      ],
      "metadata": {
        "id": "-5fIu3iEYWhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Passwords Generation**"
      ],
      "metadata": {
        "id": "9w5KdRkPVIU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "def generate_password(model, temperature, all_chars, char_to_int, device, int_to_char):\n",
        "    model.eval()\n",
        "    hidden = model.init_hidden()\n",
        "\n",
        "    start_str = random.choice(all_chars)\n",
        "    predict_len = random.randint(6, 15)\n",
        "\n",
        "    input_tensor = char_tensor(start_str)\n",
        "    predicted_str = start_str\n",
        "    last_char = input_tensor[-1]\n",
        "\n",
        "    for p in range(predict_len):\n",
        "        output, hidden = model(last_char.unsqueeze(0).to(device), hidden)\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        # Clamp the output distribution to avoid 'inf' and 'nan'\n",
        "        output_dist = torch.clamp(output_dist, min=0.0001, max=1.0)\n",
        "        output_dist = output_dist / torch.sum(output_dist)\n",
        "        top_char = torch.multinomial(output_dist, 1)[0]\n",
        "\n",
        "        predicted_char = int_to_char[top_char.item()]\n",
        "        predicted_str += predicted_char\n",
        "        last_char = char_tensor(predicted_char).to(device)\n",
        "\n",
        "    return predicted_str\n",
        "\n",
        "def generate_passwords_to_file(model, num_pwd, file_path, temperature, all_chars, char_to_int, device, int_to_char):\n",
        "    model.to(device)\n",
        "    with open(file_path, 'w') as file:\n",
        "        for _ in range(num_pwd):\n",
        "            password = generate_password(model, temperature, all_chars, char_to_int, device, int_to_char)\n",
        "            file.write(password + '\\n')\n",
        "\n",
        "# Example usage:\n",
        "num_pwd = 1000000\n",
        "output_file_path = '/content/drive/My Drive/generated1M_GRU_passwords.txt'  # Replace with your desired file path\n",
        "generate_passwords_to_file(gru, num_pwd, output_file_path, 0.85, all_chars, char_to_int, device, int_to_char)\n"
      ],
      "metadata": {
        "id": "SyUX2VduTn4B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "155fc114-9092-4d4f-ea38-7d856d6804b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[Errno 107] Transport endpoint is not connected",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-a55752a53db1>\u001b[0m in \u001b[0;36mgenerate_passwords_to_file\u001b[0;34m(model, num_pwd, file_path, temperature, all_chars, char_to_int, device, int_to_char)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mpassword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_password\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_to_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpassword\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 107] Transport endpoint is not connected",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;31mOSError\u001b[0m: [Errno 107] Transport endpoint is not connected",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-a55752a53db1>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mnum_pwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0moutput_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/generated1M_GRU_passwords.txt'\u001b[0m  \u001b[0;31m# Replace with your desired file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mgenerate_passwords_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.85\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_to_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-a55752a53db1>\u001b[0m in \u001b[0;36mgenerate_passwords_to_file\u001b[0;34m(model, num_pwd, file_path, temperature, all_chars, char_to_int, device, int_to_char)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_passwords_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_to_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_pwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mpassword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_password\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_to_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 107] Transport endpoint is not connected"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Calculate Accuracy**"
      ],
      "metadata": {
        "id": "EZiZw-0QVK9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy_and_matches(original_dataset_path, generated_file_path):\n",
        "    # Load the original dataset\n",
        "    with open(original_dataset_path, 'r') as file:\n",
        "        original_passwords = set(file.read().splitlines())\n",
        "\n",
        "    # Load generated passwords\n",
        "    with open(generated_file_path, 'r') as file:\n",
        "        generated_passwords = file.read().splitlines()\n",
        "\n",
        "    # Find matches\n",
        "    matches = [password for password in generated_passwords if password in original_passwords]\n",
        "    dub = set(matches)\n",
        "    # Calculate accuracy based on full line matches\n",
        "    accuracy = (len(dub) / len(generated_passwords)) * 100 if generated_passwords else 0\n",
        "    return accuracy, matches\n",
        "\n",
        "# Paths to the files\n",
        "original_dataset_path = '/content/drive/My Drive/Ashley-Madison.txt'  # Modify as needed\n",
        "generated_file_path = '/content/drive/My Drive/generated1M_GRU_passwords.txt'  # Modify as needed\n",
        "\n",
        "# Calculate accuracy and get matches\n",
        "accuracy, matching_passwords = calculate_accuracy_and_matches(original_dataset_path, generated_file_path)\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "print(\"Matching Passwords:\", set(matching_passwords))"
      ],
      "metadata": {
        "id": "-7wK9eY3VOef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99796421-87a2-446a-e53e-eec1b4bbd570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.00%\n",
            "Matching Passwords: {'7123890'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(gru.state_dict(), '/content/drive/My Drive/gru_model.pth')\n",
        "# Ensure your GRU model architecture is defined as `gru` here\n",
        "gru.load_state_dict(torch.load('/content/drive/My Drive/gru_model.pth'))\n",
        "gru.eval()  # Set the model to evaluation mode\n"
      ],
      "metadata": {
        "id": "opX6p--ktUZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aa44383-9f72-4f5f-fadb-c157a799f439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GRU(\n",
              "  (embedding): Embedding(90, 64)\n",
              "  (gru): GRU(64, 64, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (fc): Linear(in_features=64, out_features=90, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}